{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3SkDSWJ0dYJ"
   },
   "source": [
    "# Objective <a class=\"anchor\" id=\"inicio\"></a>\n",
    "\n",
    "In this notebook, we will explore a way to implement tabular Temporal Difference (TD) methods, which we will use to solve a variety of task environments. Multiple experiments will be conducted to evaluate the strengths of each algorithm across different settings.\n",
    "\n",
    "This is a complementary material for the [slides on temporal difference](https://github.com/EAndrade-Lotero/BasicTabularRL/blob/main/slides/BasicTabularRL.pdf). I recommend reading these first and then come back to this notebook.\n",
    "\n",
    "This notebook is based on the presentation by Sanghi (2021), Chapter 5, and its [notebooks](https://github.com/Apress/deep-reinforcement-learning-python), as well as Sutton and Barto (2018), Chapter 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to Exercise 1](#ej1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "When starting the notebook or restarting the kernel, you can load all the dependencies for this notebook by running the following cells. This is also the place to install any missing dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Python:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install requirements in linux or mac\n",
    "#!pip3 install -r requirements.txt\n",
    "#!pip3 install gymnasium[toy-text]\n",
    "\n",
    "# Uncomment to install requirements in windows\n",
    "#!python -m pip install -r requirements.txt\n",
    "#!python -m pip installgymnasium[toy-text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from termcolor import colored, cprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For this notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ambientes import GridworldEnv, CliffworldEnv\n",
    "from agents import Agent\n",
    "from algoritmos import *\n",
    "from utils import Episode, Experiment\n",
    "from plot_utils import PlotGridValues, Plot\n",
    "from tests import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5HnQ_gA70dYL"
   },
   "source": [
    "# Sections\n",
    "\n",
    "Our plan is as follows:\n",
    "\n",
    "* [Method for policy evaluation](#eval)\n",
    "* [Control methods](#control)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method for policiy evaluation <a class=\"anchor\" id=\"eval\"></a>\n",
    "    \n",
    "([Back to top](#inicio))\n",
    "\n",
    "Let us recall that the first step in solving a task environment is addressing the evaluation problem. Here, the goal is to determine the value of a state given a policy. In other words, we want to estimate the expected value of the total discounted reward when following a policy starting from a state $s$. This needs to be done for each state $s$ in the task environment. We will examine the Temporal Difference (TD) method to tackle this problem, using the GridWorld environment as an example.\n",
    "\n",
    "We begin by defining a policy for a $4\\times 4$ grid world (note that this is the same policy we worked with in the previous notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (4,4)\n",
    "env = GridworldEnv(shape=shape)\n",
    "policy = ([env.NORTH] + [env.EAST]*(shape[0] - 2) + [env.SOUTH]) * shape[1]\n",
    "pp = PlotGridValues(shape=shape, dict_acciones=env.dict_acciones)\n",
    "pp.plot_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know the state values. We will use the dynamic programming method to compute them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = policy_eval(env, policy)\n",
    "pp.plot_policy_and_values(policy, V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Temporal Difference (TD) evaluation method, it is not necessary to wait for the entire episode to end before updating our value estimates for the visited states. Instead, at each step, we update the value estimate of the most recently visited state by bootstrapping with the values stored in memory. The pseudocode for the algorithm is as follows:\n",
    "\n",
    "<img src=\"./imagenes/td_evaluationb.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej1\"></a>**Exercise 1:**  \n",
    "\n",
    "([Next exercise](#ej2))\n",
    "\n",
    "Implement lines 5 to 8 from the pseudocode above in the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_0_evaluation(env, policy:np.array, alfa:float=0.1, gama:float=1, max_iter:int=500, max_steps=1000, V:np.array=None) -> np.array:\n",
    "    '''\n",
    "    Método de diferencia temporal para estimar el valor de los \n",
    "    estados de un MDP generando una muestra de episodios con base en una política dada. \n",
    "    Input:\n",
    "        - env, un ambiente con atributos nA, nS, shape \n",
    "               y métodos reset(), step()\n",
    "        - policy, una política determinista, policy[state] = action\n",
    "        - alfa, real con el parámetro de step-size\n",
    "        - gama, con el parámetro de factor de descuento\n",
    "        - max_iter, entero con la cantidad máxima de episodios\n",
    "        - max_steps, entero con la cantidad máxima de pasos\n",
    "        - Opcional: V, un np.array que por cada s devuelve su valor estimado\n",
    "    Output:\n",
    "        - V, un np.array que por cada s devuelve su valor estimado\n",
    "    '''\n",
    "    if V is None:\n",
    "        V = np.zeros(env.nS)\n",
    "    for _ in range(max_iter):\n",
    "        state = np.random.randint(env.nS)\n",
    "        env.state = state\n",
    "        done = False\n",
    "        counter = 0\n",
    "        while not done:\n",
    "            pass\n",
    "            # AQUÍ SU CÓDIGO\n",
    "            \n",
    "            # HASTA AQUÍ SU CÓDIGO\n",
    "            counter += 1\n",
    "            if counter > max_steps:\n",
    "                break\n",
    "    return V \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your answer using the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "V = td_0_evaluation(env, policy)\n",
    "VV = np.flipud(np.reshape(V, shape))\n",
    "test = np.array([[ 0.,         -4.36081499, -3.97308306, -2.99943748],\n",
    "                 [-0.99993144, -3.77424704, -2.99432731, -1.99999999],\n",
    "                 [-1.99314462, -2.7058518,  -1.99561459, -1.   ],\n",
    "                 [-2.85262296, -1.7706891, -0.99695675,  0.   ]])\n",
    "assert(np.all(np.isclose(VV, test)))\n",
    "print('¡Correcto!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Methods <a class=\"anchor\" id=\"control\"></a>\n",
    "\n",
    "([Back to top](#inicio))\n",
    "\n",
    "Solving an environment involves finding the optimal policy that the agent should follow to maximize its utility. In the grid world, the optimal policy is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (4,4)\n",
    "env = GridworldEnv(shape=shape)\n",
    "policy = value_iteration(env, discount_factor=1, theta=0.01, verbose=0)\n",
    "print('Política óptima:')\n",
    "pp = PlotGridValues(shape=shape, dict_acciones=env.dict_acciones)\n",
    "pp.plot_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal policy, the Temporal Difference (TD) method maintains an estimate of the state-action value for the policy in memory. Each time this estimate is updated, the policy is also improved using an $\\epsilon$-greedy improvement strategy:\n",
    "\n",
    "$$\n",
    "\\pi_{k+1}(a|s) = \\begin{cases}\n",
    "1-\\epsilon &\\text{if }a=\\text{arg}\\!\\max_{a'} Q_{\\pi_k}(s,a') \\cr\n",
    "\\frac{\\epsilon}{\\#\\text{actions}-1} &\\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Episode` Class\n",
    "\n",
    "To run simulations in an organized and straightforward manner, we have implemented the `Episode` class, which is located in the `utils.py` module. Let's observe a random algorithm acting in the grid world environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "shape = (3,3)\n",
    "env = GridworldEnv(shape=shape)\n",
    "# Create agent\n",
    "parameters = {\\\n",
    "    'nS': np.prod(shape),\\\n",
    "    'nA': 4,\\\n",
    "    'gamma': 1,\\\n",
    "    'epsilon': 0.1,\\\n",
    "}\n",
    "agent = Agent(parameters=parameters)\n",
    "# Create episode\n",
    "episode = Episode(environment=env, \\\n",
    "                  env_name='GW', \\\n",
    "                  agent=agent, \\\n",
    "                  model_name='Random', \\\n",
    "                  num_rounds=3, \\\n",
    "                )\n",
    "# Run and show information\n",
    "df = episode.run(verbose=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have executed the `run()` method, which runs a single episode, with the `verbose=4` option. This setting causes the method to print detailed information for each step of the episode. If we do not need this information, we can set `verbose=0` (or omit this argument entirely, as it is optional with a default value of 0) to avoid unnecessary delays in data processing.\n",
    "\n",
    "Now, let’s examine a graphical representation of the evolution of total rewards over multiple episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "shape = (4,4)\n",
    "env = GridworldEnv(shape=shape)\n",
    "# Create agent\n",
    "parameters = {\\\n",
    "    'nS': np.prod(shape),\\\n",
    "    'nA': 4,\\\n",
    "    'gamma': 1,\\\n",
    "    'epsilon': 0.1,\\\n",
    "}\n",
    "agent = Agent(parameters=parameters)\n",
    "# Create episode\n",
    "episode = Episode(environment=env, \\\n",
    "                  env_name='GW', \\\n",
    "                  agent=agent, \\\n",
    "                  model_name='Random', \\\n",
    "                  num_rounds=100\n",
    "                )\n",
    "# Train agent\n",
    "df = episode.simulate(num_episodes=50, verbose=0)\n",
    "# Plot rewards\n",
    "Plot(df).plot_rewards()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we observe the evolution of rewards over 50 episodes, each consisting of a maximum of 100 steps. For this, we used the `simulate()` method.\n",
    "\n",
    "We can inspect the agent’s policy by visualizing its `policy` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = episode.agent.policy\n",
    "policy = [np.argmax(p[s,]) for s in range(env.nS)]\n",
    "pp = PlotGridValues(shape=shape, dict_acciones=env.dict_acciones)\n",
    "print('Política del agente:')\n",
    "pp.plot_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `Experiment` Class**\n",
    "\n",
    "One way to use the `Experiment` class is to test the already-trained agent without requiring further exploration. We can apply the `run_experiment()` method on the agent and plot a histogram of the total rewards per episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment\n",
    "experiment = Experiment(environment=env,\\\n",
    "                        env_name='GW', \\\n",
    "                        num_rounds=10, \\\n",
    "                        num_episodes=10, \\\n",
    "                        num_simulations=10)\n",
    "# Test agent\n",
    "agents = experiment.run_experiment(agents=[agent],\\\n",
    "                                  names=['Random'], \\\n",
    "                                  measures=['hist_reward'], \\\n",
    "                                  learn=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Difference Control <a class=\"anchor\" id=\"TDcontrol\"></a>\n",
    "\n",
    "([Back to Control](#control))\n",
    "\n",
    "We will implement two agents: one using the SARSA learning rule and the other using the Q-learning rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej2\"></a>**Exercise 2:**  \n",
    "\n",
    "([Previous exercise](#ej1)) ([Next exercise](#ej3))\n",
    "\n",
    "Implement the SARSA agent according to the SARSA agent pseudocode:\n",
    "\n",
    "<img src=\"./imagenes/sarsa_agent.png\" width=\"500\"/>\n",
    "\n",
    "Use the following cell to implement the agent. The focus should be on implementing line 5, where you calculate the update using bootstrapping (estimate), the temporal difference error (delta), and then update the previous value by moving it toward delta by a fraction $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA(Agent) :\n",
    "    '''\n",
    "    Implements a SARSA learning rule.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, parameters:dict):\n",
    "        super().__init__(parameters)\n",
    "        self.alpha = self.parameters['alpha']\n",
    "        self.debug = False\n",
    "   \n",
    "    def update(self, next_state, reward, done):\n",
    "        '''\n",
    "        Agent updates its model.\n",
    "        '''\n",
    "        # obtain previous state\n",
    "        state = self.states[-1]\n",
    "        # obtain previous action\n",
    "        action = self.actions[-1]\n",
    "        # Get next_action\n",
    "        next_action = self.make_decision()\n",
    "        # Find bootstrap\n",
    "        estimate = ... # recompensa más descuento por valor del siguiente estado\n",
    "        # Obtain delta\n",
    "        delta = ... # Diferencia temporal: estimado menos valor del estado actual\n",
    "        # Update Q value\n",
    "        prev_Q = self.Q[state, action]\n",
    "        self.Q[state, action] = ... # Actualizar en la dirección de delta por una fracción alfa\n",
    "        # Update policy\n",
    "        self.update_policy(state)\n",
    "        if self.debug:\n",
    "            print('')\n",
    "            print(dash_line)\n",
    "            print(f'Learning log:')\n",
    "            print(f'state:{state}')\n",
    "            print(f'action:{action}')\n",
    "            print(f'reward:{reward}')\n",
    "            print(f'estimate:{estimate}')\n",
    "            print(f'Previous Q:{prev_Q}')\n",
    "            print(f'delta:{delta}')\n",
    "            print(f'New Q:{self.Q[state, action]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your code by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (3,4)\n",
    "# Create agent\n",
    "parameters = {\\\n",
    "    'nS': np.prod(shape),\\\n",
    "    'nA': 4,\\\n",
    "    'gamma': 1,\\\n",
    "    'epsilon': 0.1,\\\n",
    "    'alpha': 0.1, \\\n",
    "}\n",
    "agent_SARSA = SARSA(parameters=parameters)\n",
    "test = test_sarsa(agent_SARSA)\n",
    "if test:\n",
    "    cprint(colored('¡Test superado!', 'green'))\n",
    "else:\n",
    "    cprint(colored('¡Implementación incorrecta!', 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej3\"></a>**Exercise 3:**  \n",
    "\n",
    "([Previous exercise](#ej2)) ([Next exercise](#ej4))\n",
    "\n",
    "Implement the agent using the Q-learning rule, following the pseudocode below:\n",
    "\n",
    "<img src=\"./imagenes/q_learning_agent.png\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_learning(Agent) :\n",
    "    '''\n",
    "    Implements a Q-learning rule.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, parameters:dict):\n",
    "        super().__init__(parameters)\n",
    "        self.alpha = self.parameters['alpha']\n",
    "        self.debug = False\n",
    "   \n",
    "    def update(self, next_state, reward, done):\n",
    "        '''\n",
    "        Agent updates its model.\n",
    "        '''\n",
    "        # obtain previous state\n",
    "        state = ... # Aquí estado previo\n",
    "        # obtain previous action\n",
    "        action = self.actions[-1]\n",
    "        # Find bootstrap\n",
    "        maxQ = self.max_Q(next_state) \n",
    "        estimate = ... # Calcula el estimado\n",
    "        # Obtain delta\n",
    "        delta = ... # Calcula el delta\n",
    "        # Update Q value\n",
    "        prev_Q = self.Q[state, action]\n",
    "        self.Q[state, action] = ... # Actualiza el valor\n",
    "        # Update policy\n",
    "        self.update_policy(...) # Actualizar la política en el estado        \n",
    "        if self.debug:\n",
    "            print('')\n",
    "            print(dash_line)\n",
    "            print(f'Learning log:')\n",
    "            print(f'state:{state}')\n",
    "            print(f'action:{action}')\n",
    "            print(f'reward:{reward}')\n",
    "            print(f'estimate:{estimate}')\n",
    "            print(f'Previous Q:{prev_Q}')\n",
    "            print(f'delta:{delta}')\n",
    "            print(f'New Q:{self.Q[state, action]}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corra la siguiente celda para verificar su implementación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (3,4)\n",
    "# Create agent\n",
    "parameters = {\\\n",
    "    'nS': np.prod(shape),\\\n",
    "    'nA': 4,\\\n",
    "    'gamma': 1,\\\n",
    "    'epsilon': 0.1,\\\n",
    "    'alpha': 0.1, \\\n",
    "}\n",
    "agent_Q = Q_learning(parameters=parameters)\n",
    "test = test_q(agent_Q)\n",
    "if test:\n",
    "    cprint(colored('¡Test superado!', 'green'))\n",
    "else:\n",
    "    cprint(colored('¡Implementación incorrecta!', 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"ej4\"></a>**Exercise 4:**  \n",
    "\n",
    "([Previous exercise](#ej3)) ([Next exercise](#ej5))\n",
    "\n",
    "Compare the performance of the SARSA agent with the Q-learning agent in the cliff walking environment (implemented in the `CliffworldEnv` class within the `ambientes` module). Recall that this example was discussed in class, where it was mentioned that Q-learning does not account for occasional slips in the $\\epsilon$-greedy policy, whereas SARSA does.\n",
    "\n",
    "Use the following specifications:\n",
    "\n",
    "* Maximum number of steps: 200  \n",
    "* Number of episodes: 500  \n",
    "* Number of simulations: 10  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the resulting policies for the agents by running the following cells:\n",
    "\n",
    "**Note:** The following cells will only work after completing Exercise 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "shape = (3,4)\n",
    "env = CliffworldEnv(shape=shape)\n",
    "shape = (3,4)\n",
    "pp = PlotGridValues(shape=shape, dict_acciones=env.dict_acciones)\n",
    "sarsa = agents[0]\n",
    "p = sarsa.policy\n",
    "policy = [np.argmax(p[s,]) for s in range(env.nS)]\n",
    "policy = np.flipud(np.reshape(policy, shape))\n",
    "pp.plot_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "shape = (3,4)\n",
    "env = CliffworldEnv(shape=shape)\n",
    "shape = (3,4)\n",
    "pp = PlotGridValues(shape=shape, dict_acciones=env.dict_acciones)\n",
    "q_agent = agents[1]\n",
    "p = q_agent.policy\n",
    "policy = [np.argmax(p[s,]) for s in range(env.nS)]\n",
    "policy = np.flipud(np.reshape(policy, shape))\n",
    "pp.plot_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the learning process, we want the agent to explore different courses of action to gain greater confidence in finding an optimal policy. However, when deploying the agent in production, we want it to perform at its best. To achieve this, we need to set its $\\epsilon$ parameter to 0.\n",
    "\n",
    "Let us compare the optimal performance of the SARSA and Q-learning agents without exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down exploration\n",
    "agents[0].epsilon = 0\n",
    "agents[1].epsilon = 0\n",
    "for s in range(env.nS):\n",
    "    agents[0].update_policy(s)\n",
    "    agents[1].update_policy(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment\n",
    "experiment = Experiment(environment=env,\\\n",
    "                 env_name='Cliff', \\\n",
    "                 num_rounds=100, \\\n",
    "                 num_episodes=100, \\\n",
    "                 num_simulations=1)\n",
    "# Use stored agents to run test\n",
    "experiment.run_experiment(\n",
    "                agents=agents,\\\n",
    "                names=['SARSA', 'Q_learning'], \\\n",
    "                measures=['hist_reward'],\\\n",
    "                learn=False)\n",
    "print('¡Listo!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Takeaways\n",
    "\n",
    "* Evaluate a policy using Temporal Difference methods.  \n",
    "* Improve a policy using SARSA and Q-learning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "([Back to top](#inicio))\n",
    "\n",
    "Shanghi, N. (2021) Deep Reinforcement Learning with Python: With PyTorch, TensorFlow and OpenAI Gym. Apress. \n",
    "\n",
    "Sutton R., & Barto, A., (2015) Reinforcement Learning: An Introduction, 2nd Edition. A Bradford Book. Series: Adaptive Computation and Machine Learning series. \n",
    "\n",
    "Winder, P., (2021) Reinforcement Learning: Industrial Applications of Intelligent Agents. O’Relly."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "arboles_busqueda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
